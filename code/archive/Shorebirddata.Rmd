---
title: "5/17/2021 shorebird data"
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r packages}
library(readxl)
library(tidyverse)
library(janitor)
library(ggdark)
library(Hmisc)
library(broom)
library(viridis)
library(ggplot2)
library(car) #levenes
library(dunn.test) #dun test
library(rcompanion)#for cldList
library(nlme)
library(ggpubr) #ggdensity

#want to identify when peak visitation happens- day of week, weekend/weekday, season, month. Is visitation higher on Dec weekends than July weekends?
#is visitation higher when weather is nicer? Most imp factors in determining visitation- multivariate analysis. Generalized additive model? Multiple regression- how strong of relationship temp vs visitation is? Use with shorebird and trail counter data. 
#What % of people that visit coastal trail end up in tidepools? Trail counts vs people survey. Connect timings between trail counts and people survey.
```

#combine temp data files- cut down columns and make column names the same in excel
```{r}
#Took files from LP's code "SD_Weather_Abiotic_Query.R". Deleted all columns except date and dry bulb temp for each year file. Put in folder "SD_Weather_Data"

#setwd("~/Desktop/CNP_internship/LS_Data/SD_Weather_Data")

  # use list apply to read in and rbind (stack) files
temp <- do.call(
         # merge read files (filling in uneven column #'s with NAs)
          plyr::rbind.fill,
          # read csv files in working directory
          lapply(list.files(), read_csv))

```

```{r shorebird}
#setwd("~/Desktop/CNP_internship/LS_Data")

# tide predictions from tbone tides
tide <- read_csv("tbone_tides_hourly_1990_2026.csv", 
                 col_types = cols(datetime = col_datetime(format = "%m/%d/%Y %H:%M")))


shorebird<-read_excel("Shorebird_People_Data/Shorebird_People_Data.xlsx")

#temp$SurveyDate<-temp$date
#temp$SurveyDate<-as.Date(temp$SurveyDate)
#temp<-temp[,-1]
#str(temp)
temp$SurveyDate<-temp$date

#remove 's' in temp

#average temps per day
tempaverage<-temp %>% 
  na.omit()%>%
  mutate(SurveyDate=date(SurveyDate),
         hourlydrybulbtemperature=as.numeric(gsub("s","",hourlydrybulbtemperature)))%>% #remove 's' from temp and make numeric
  group_by(SurveyDate) %>% 
  summarise(meantemp = mean(hourlydrybulbtemperature))

#add temps to people data
tempdata<-left_join(people, tempaverage, by="SurveyDate")

tempdata$Year<-as.numeric(tempdata$Year)

#add column with just years
shorebird$Year=format(shorebird$SurveyDate, "%Y")
unique(shorebird$Year)
#filter for just people
people<- shorebird %>% 
  filter(DataType=="People")#%>%
 # mutate(SurveyDate=date(SurveyDate))

```

#separated by year and zone class
```{r}
#get average counts per year
average<-people %>% 
  group_by(SurveyDate, ZoneClass) %>% 
  summarise(MeanCount = mean(DataCount))%>%
  na.omit()

#graph counts facet separated by Zone
ggplot(average)+
  geom_point(mapping=aes(SurveyDate, MeanCount))+
  facet_grid(~ZoneClass)+
  theme_bw()
glimpse(average)
#try to change year to numeric instead of character to graph lines
average$Year<-as.numeric(average$Year)
#graph counts by year colored by Zone
ggplot(average, aes(SurveyDate, MeanCount, color=ZoneClass))+
  geom_point()+
  theme_classic()+
  #geom_line()+
  geom_smooth()+
  labs(y="Average number of people per day", color="Zone")#+
  #scale_x_discrete(limits=c("1990","1992","1994","1996","1998","2000", "2002","2004","2006","2008","2010","2012","2014","2016","2018","2020","2022"))+
  theme(axis.text.x = element_text(size=8, angle=45))
  #scale_fill_discrete(labels = c("Zone I", "Zone II", "Zone III"))
ggsave("people vs year and zone.png")

#same but without points
ggplot(average, aes(SurveyDate, MeanCount, color=ZoneClass))+
  theme_classic()+
  geom_smooth()+
  labs(y="Average number of people per day", color="Zone")
ggsave("people vs year and zone line.png")


```

#ANCOVA- does slope differ? does zone affect people count over time?
```{r}
average$ZoneClass<-as.factor(average$ZoneClass)
average$Year<-as.numeric(average$Year)
#year as numeric
m<-lm(MeanCount~Year*ZoneClass, data=average)
#add interaction and see if significant (if slopes differ)

#, random=~1+Year|ZoneClass
# test assumptions----------------------
# test fit of the residuals.  There should be no patterns
par(mfrow=c(2,2)) # make the subplots
qqnorm(resid(m))
E2<-resid(m, type = "normalized") # extract normalized residuals
F2<-fitted(m) # extract the fitted data
plot(F2, E2, xlab = "fitted values", ylab = "residuals") # plot the relationship
abline(h = 0, lty = 2) # add a flat line at zerp

# test for homogeneity of variances
boxplot(E2~average$ZoneClass, ylab = "residuals")

# check for independence. There should be no pattern
plot(E2~average$Year, ylab = 'residuals', xlab = "Year")

#residuals look good?

anova(m) #year pvalue=0.0002 significant linear relationship
summary(m)

## pull out the random and fixed effects
ranef(m) # random
fixef(m) # fixed

# Plot the data
par(mfrow=c(1,1)) # only plot the data in one subplot
plot(average$Year, average$MeanCount, col=average$ZoneClass, xlab = 'Year', ylab = 'People')

# extract the fitted values to make a best fit line
F0<-fitted(m, level = 0) # fitted across sites
F1<-fitted(m, level = 1) # fitted between sites

# we need to put the data in order so that we can actually create a line
I<-order(average$Year)
# plot the best line for the entire model
lines(average$Year[I], F0[I],  lwd = 3, xlim=range(average$Year), ylim = range(average$MeanCount))

#Now, we can look at the best fit line for the entire model.

# create an array of unique sites
Sites<-unique(average$ZoneClass)

# plot the best line with the random intercepts
for (i in 1:length(Sites)){
  x1<-average$Year[average$ZoneClass==Sites[i]] # x value at site i
  y1<-F1[average$ZoneClass==Sites[i]] # y value at site i
  ind<-order(x1) # reorder the data
  lines(x1[ind],y1[ind], col = Sites[i]) # make the plot
}
# add a legend
legend("topleft",legend= Sites, col=c(1,2,3,4), pch=21, bty="n")

ggplot(average, aes(Year, MeanCount, group = ZoneClass, color=ZoneClass))+
  geom_smooth(method = 'lm')+
  geom_point()+


```

#ANCOVA attempt 2
```{r}
mod1<-

```

#KW test if sig diff between zones across years
```{r}
#test assumptions
#normality
shapiro.test(average$MeanCount[average$ZoneClass=="I"]) 
#pvalue=0.1279- normal

shapiro.test(average$MeanCount[average$ZoneClass=="II"])
#pvalue=0.000254 not normal

shapiro.test(average$MeanCount[average$ZoneClass=="III"])
#pvalue=1.385e-7 not normal

#levene's instead of bartlett for variance b/c not normal
leveneTest(MeanCount~ZoneClass, data=average) #pvalue=5.518e-10 not equal variances

#Kruskal-wallis since doesn't meet assumptions
kruskal.test(MeanCount~ZoneClass, data=average)
#pvalue=<2.2e-16 so there is sig diff between zones across years.
```

#separated by year
```{r}
#get average counts per year
averageyear<-people %>% 
  group_by(Year) %>% 
  summarise(MeanCount = mean(DataCount))
str(averageyear)
averageyear$Year<-as.numeric(averageyear$Year)

#graph all counts with line
ggplot(averageyear, aes(Year, MeanCount))+
  geom_point()+
  ylab("Average number of people/day")+
  geom_smooth(method="lm")+
  theme_bw()

yearlm<-lm(MeanCount~Year, data=averageyear)
summary(yearlm) #pvalue=0.000565 sig
```

#separated by zone, all sig different
```{r}
#graph all counts separated by zone
ggplot(people, aes(ZoneClass, DataCount))+
  geom_boxplot()+
  #geom_smooth()+
  theme_bw()+
  labs(x="Zone", y="Number of people")

#test assumptions
#normality
shapiro.test(people$DataCount[people$ZoneClass=="I"]) 
#pvalue=<2.2e-16- not normal

shapiro.test(people$DataCount[people$ZoneClass=="II"])
#pvalue=<2.2e-16- not normal

shapiro.test(people$DataCount[people$ZoneClass=="III"])
#pvalue=<2.2e-16- not normal

#levene's instead of bartlett for variance b/c not normal
leveneTest(DataCount~ZoneClass, data=people) #pvalue=<2.2e-16- not equal variances

#Kruskal-wallis since doesn't meet assumptions
ktest<-kruskal.test(DataCount~ZoneClass, data=people)
#pvalue=<2.2e-16 so there is sig diff between zones. 

library(ggsignif)
ggplot(people, aes(x=ZoneClass, y= DataCount)) +
  geom_boxplot() +
  geom_signif(comparisons = list(c("I", "II", "III")), map_signif_level=TRUE, color = "blue1", na.rm = T)
```

#subset out zone 2 and 3
```{r}
zone<-subset(people, ZoneClass!="I")
#test assumptions
#normality

shapiro.test(zone$DataCount[zone$ZoneClass=="II"])
#pvalue=<2.2e-16- not normal

shapiro.test(zone$DataCount[zone$ZoneClass=="III"])
#pvalue=<2.2e-16- not normal

#wilcox test
wilcox.test(zone$DataCount~zone$ZoneClass, data=zone, exact=F, alternative="greater")
#pvalue=<2.2e-16 so zone 2 sig more than zone 3
```

#visitation vs day of the week
```{r}
#want to identify when peak visitation happens- day of week, weekend/weekday, season, month. Is visitation higher on Dec weekends than July weekends?

#Add column that converts date to day of the week
people$day<-weekdays(as.Date(people$SurveyDate))

#graph days vs count
people%>% 
    mutate(day = fct_relevel(day, 
            "Monday", "Tuesday", "Wednesday", 
            "Thursday", "Friday", "Saturday", "Sunday")) %>%
  ggplot(aes(day, DataCount))+
  geom_boxplot()+
  #geom_smooth()+
  theme_bw()

####look at reordering in forcat package 

#test normality- none are normal
shapiro.test(people$DataCount[people$day=="Friday"]) 
shapiro.test(people$DataCount[people$day=="Monday"])
shapiro.test(people$DataCount[people$day=="Saturday"]) 
shapiro.test(people$DataCount[people$day=="Sunday"]) 
shapiro.test(people$DataCount[people$day=="Thursday"]) 
shapiro.test(people$DataCount[people$day=="Tuesday"]) 
shapiro.test(people$DataCount[people$day=="Wednesday"]) 

#test equal variances
leveneTest(DataCount~day, data=people) #pvalue=<2.2e-16 not equal variances

#non parametric ANOVA
kruskal.test(DataCount~day, data=people)

#pairwise dunn posthoc test with bonferroni correction
dunn_day <- dunn.test(x = people$DataCount, # data 
g = people$day, # group
method = 'bonferroni', # Bonferroni correction applied
kw = TRUE,
table = TRUE)

#visitation higher on weekends 

#use to put letters on plot
dunn_day <- as_tibble(dunn_day)
  
placeorder <- c('Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday')

dunn_day <- dunn_day %>%
    mutate(comparisons2 = comparisons) %>%
    separate(comparisons2, c('first', 'second'), sep = ' - ') %>%
    arrange(factor(second, levels = placeorder)) %>%
    arrange(factor(first, levels = placeorder))

  # get letters and put in summary tables 
place_letters <- cldList(
    comparison = dunn_day$comparisons, 
    p.value = dunn_day$P.adjusted,
    threshold  = 0.05)

place_letters$day <- place_letters$Group

people <- merge(place_letters, people, by = 'day')

people_daysummary <- people %>%
  group_by(day) %>%
  tally()

#graph with signficant
people%>% 
    mutate(day = fct_relevel(day, 
            "Monday", "Tuesday", "Wednesday", 
            "Thursday", "Friday", "Saturday", "Sunday")) %>%
  ggplot(aes(day, DataCount))+
  geom_boxplot()+
  #geom_smooth()+
  theme_bw()+
  geom_text(data = people, mapping = aes(fct_reorder(day, DataCount), y = 350, label = Letter), vjust = +0.9)
```

#visitation vs month of the year- weird
```{r}

#Add column that converts date to month of the year
people$month<-months(as.Date(people$SurveyDate))

#graph month vs count
people %>%
  mutate(month = fct_relevel(month, 
            "January", "February", "March", 
            "April", "May", "June", "September","October","November","December")) %>%
  ggplot( aes(x=month, y=DataCount)) +
    geom_boxplot() +
    xlab("")+
  theme_bw()

#test normality- none are normal
shapiro.test(people$DataCount[people$month=="January"]) 
shapiro.test(people$DataCount[people$month=="February"])
shapiro.test(people$DataCount[people$month=="March"]) 
shapiro.test(people$DataCount[people$month=="April"]) 
shapiro.test(people$DataCount[people$month=="May"]) 
shapiro.test(people$DataCount[people$month=="June"]) 
shapiro.test(people$DataCount[people$month=="September"]) 
shapiro.test(people$DataCount[people$month=="October"]) 
shapiro.test(people$DataCount[people$month=="November"]) 
shapiro.test(people$DataCount[people$month=="December"]) 

#test equal variances
leveneTest(DataCount~month, data=people) #pvalue=0.00149 not equal variances

#non parametric ANOVA
kruskal.test(DataCount~month, data=people) #pvalue=0.01179 sig median diff between months

#try dunn post hoc for kw

#can try chi2 tests. is distribution different between months? double check

#can be due to # of low tides below 0.7 during each month. Fewer low tides for visitation. 

#way less surveys in June
people %>%
  select(SurveyDate)%>% 
  mutate(month = month(SurveyDate))%>%
  distinct()%>%
  group_by(month)%>%
  tally()

# find # of hours for each month that are < 0.2 m. Less in summer
tide%>%
  mutate(month=month(datetime)) %>%
  mutate(hour=hour(datetime)) %>%
  filter(tidelvl<=0.21 & hour%in% c(9:17))%>%
  group_by(month)%>%
  tally()

###########################################
#try pulling 5 random samples from each month with no replacements
sub_people <- people %>% 
  group_by(month) %>% 
  sample_n(5, replace=F)

#graph it
sub_people %>%
  ggplot(aes(x=factor(month, level=c("January", "February", "March", "April", "May", "June", "September","October","November","December")), y=DataCount)) +
    geom_boxplot() +
    xlab("")+
  theme_bw()

#test normality- none are normal
shapiro.test(sub_people$DataCount[people$month=="January"]) #not 
shapiro.test(sub_people$DataCount[people$month=="February"])#normal
shapiro.test(sub_people$DataCount[people$month=="March"]) #not
shapiro.test(sub_people$DataCount[people$month=="April"]) #normal
shapiro.test(sub_people$DataCount[people$month=="May"]) #error
shapiro.test(sub_people$DataCount[people$month=="June"]) #error
shapiro.test(sub_people$DataCount[people$month=="September"]) 
shapiro.test(sub_people$DataCount[people$month=="October"]) #normal
shapiro.test(sub_people$DataCount[people$month=="November"]) #error
shapiro.test(sub_people$DataCount[people$month=="December"]) #not 

#test equal variances
leveneTest(DataCount~month, data=sub_people) #pvalue=0.6988 equal variances

#non parametric ANOVA
kruskal.test(DataCount~month, data=sub_people) #pvalue=0.0849 sig median diff between months in subset but much smaller sample size. 

#########################
#sum of people across dates in a month+year combo/# of surveys per month and year combo 
#add month column
average<-people %>% 
  group_by(Year, month) %>% 
  summarise(MeanCount = mean(DataCount))%>%
  
  #plus total number of people
  
#graph will have 12 points per month, june=2. Redo KW

```

#by temp- no significance
```{r}
ggplot(tempdata, aes(meantemp, DataCount, color=ZoneClass))+
  geom_point()+
  #make zoneclass factor
#fct_rev to put blue in front

shapiro.test(tempdata$DataCount) #not normal distribution pvalue=<2.2e-16

#histo of # of days at each temp

ggdensity(tempdata$DataCount, 
          main = "Density plot data count")

#multiple regression

#ANCOVA
m<-lm(DataCount~meantemp*ZoneClass, data=tempdata)

# test assumptions----------------------
# test fit of the residuals.  There should be no patterns
par(mfrow=c(2,2)) # make the subplots
qqnorm(resid(m))
E2<-resid(m, type = "normalized") # extract normalized residuals
F2<-fitted(m) # extract the fitted data
plot(F2, E2, xlab = "fitted values", ylab = "residuals") # plot the relationship
abline(h = 0, lty = 2) # add a flat line at zerp

# test for homogeneity of variances
boxplot(E2~tempdata$ZoneClass, ylab = "residuals")

# check for independence. There should be no pattern
plot(E2~tempdata$Year, ylab = 'residuals', xlab = "Year")

#residuals look good?

anova(m) #year pvalue=0.0002 significant linear relationship
summary(m)

## pull out the random and fixed effects
ranef(m) # random
fixef(m) # fixed

# Plot the data
par(mfrow=c(1,1)) # only plot the data in one subplot
plot(tempdata$meantemp, tempdata$DataCount, col=tempdata$ZoneClass, xlab = 'Temp', ylab = 'People')

# extract the fitted values to make a best fit line
F0<-fitted(m, level = 0) # fitted across sites
F1<-fitted(m, level = 1) # fitted between sites

# we need to put the data in order so that we can actually create a line
I<-order(tempdata$ZoneClass)
# plot the best line for the entire model
lines(tempdata$ZoneClass[I], F0[I],  lwd = 3, xlim=range(tempdata$ZoneClass), ylim = range(tempdata$MeanCount))

#Now, we can look at the best fit line for the entire model.

# create an array of unique sites
Sites<-unique(tempdata$ZoneClass)

# plot the best line with the random intercepts
for (i in 1:length(Sites)){
  x1<-tempdata$Year[tempdata$ZoneClass==Sites[i]] # x value at site i
  y1<-F1[tempdata$ZoneClass==Sites[i]] # y value at site i
  ind<-order(x1) # reorder the data
  lines(x1[ind],y1[ind], col = Sites[i]) # make the plot
}
# add a legend
legend("topleft",legend= Sites, col=c(1,2,3,4), pch=21, bty="n")

```
